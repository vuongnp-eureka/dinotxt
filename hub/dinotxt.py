# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This software may be used and distributed in accordance with
# the terms of the DINOv3 License Agreement.

import math
from typing import Any, Tuple, Optional
from pathlib import Path

import torch
from torch import nn

from .backbones import dinov3_vitl16


# returns dinotxt model and tokenizer
def dinov3_vitl16_dinotxt_tet1280d20h24l(
    *,
    pretrained: bool = True,
    weights: str = None,  # Local path to dinotxt weights file
    backbone_weights: str = None,  # Local path to backbone weights file
    bpe_path: str = None,  # Local path to BPE vocab file
) -> Tuple[nn.Module, Any]:
    from ..eval.text.dinotxt_model import DINOTxt, DINOTxtConfig
    from ..eval.text.text_transformer import TextTransformer
    from ..eval.text.tokenizer import get_tokenizer

    dinotxt_config = DINOTxtConfig(
        embed_dim=2048,
        vision_model_freeze_backbone=True,
        vision_model_train_img_size=224,
        vision_model_use_class_token=True,
        vision_model_use_patch_tokens=True,
        vision_model_num_head_blocks=2,
        vision_model_head_blocks_drop_path=0.3,
        vision_model_use_linear_projection=False,
        vision_model_patch_tokens_pooler_type="mean",
        vision_model_patch_token_layer=1,
        text_model_freeze_backbone=False,
        text_model_num_head_blocks=0,
        text_model_head_blocks_is_causal=False,
        text_model_head_blocks_drop_prob=0.0,
        text_model_tokens_pooler_type="argmax",
        text_model_use_linear_projection=True,
        init_logit_scale=math.log(1 / 0.07),
        init_logit_bias=None,
        freeze_logit_scale=False,
    )
    if pretrained:
        if backbone_weights is None:
            raise ValueError(
                "backbone_weights is required when pretrained=True. "
                "Please provide a local path to the backbone weights file, "
                "e.g., '/path/to/dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth'"
            )
        if weights is None:
            raise ValueError(
                "weights is required when pretrained=True. "
                "Please provide a local path to the dinotxt weights file, "
                "e.g., '/path/to/dinov3_vitl16_dinotxt_vision_head_and_text_encoder-a442d8f5.pth'"
            )
        if bpe_path is None:
            raise ValueError(
                "bpe_path is required. "
                "Please provide a local path to the BPE vocab file, "
                "e.g., '/path/to/bpe_simple_vocab_16e6.txt.gz'"
            )
    
    vision_backbone = dinov3_vitl16(pretrained=pretrained, weights=backbone_weights)
    text_backbone = TextTransformer(
        context_length=77,
        vocab_size=49408,
        dim=1280,
        num_heads=20,
        num_layers=24,
        ffn_ratio=4,
        is_causal=True,
        ls_init_value=None,
        dropout_prob=0.0,
    )
    model = DINOTxt(model_config=dinotxt_config, vision_backbone=vision_backbone, text_backbone=text_backbone)
    if pretrained:
        model.visual_model.backbone = vision_backbone
        model.eval()
        weights_path = Path(weights).expanduser().resolve()
        if not weights_path.exists():
            raise FileNotFoundError(f"DINOTxt weights file not found: {weights_path}")
        vision_head_and_text_encoder_state_dict = torch.load(weights_path, map_location="cpu")
        model.load_state_dict(vision_head_and_text_encoder_state_dict, strict=False)
    else:
        model.init_weights()
    return model, get_tokenizer(bpe_path=bpe_path)

